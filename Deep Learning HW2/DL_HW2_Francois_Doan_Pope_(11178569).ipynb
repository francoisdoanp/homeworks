{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL HW2 - Francois Doan-Pope (11178569).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Ik0Ic8_bz7Pb",
        "TxYrV6vPndiP",
        "Ga3hZMqvifpl",
        "gQkWyN0qh_Be",
        "FNrZfvsAmNoG",
        "3B6py36HjtO-",
        "xkX7j9oRlT24",
        "eN3HuZrPmABe",
        "TFS3fQvam16U"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOOSQNk5uRBhwr/C6aFURCe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francoisdoanp/projects/blob/master/DL_HW2_Francois_Doan_Pope_(11178569).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdNGnf1j_jr2",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning Homework 2\n",
        "\n",
        "By: Francois Doan-Pope\n",
        "\n",
        "Student ID: 11178569\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AavaP0QccN7",
        "colab_type": "text"
      },
      "source": [
        "# Details\n",
        "\n",
        "For this homework, I have tried three different libraries. The first one (Stellargraph) is the one that was used to get my score on the leaderboard. \n",
        "\n",
        "Some other models were tried, but were not featured in the code.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvBcfWYNdM17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# General imports\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0V2ad5bdldG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Used to export prediction file\n",
        "\n",
        "\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('drive')\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HWQUm0KdzdM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# General data import\n",
        "\n",
        "# Defining files path\n",
        "\n",
        "base_url = \"https://raw.githubusercontent.com/francoisdoanp/homeworks/master/Deep%20Learning%20HW2/\"\n",
        "\n",
        "# Importing files\n",
        "\n",
        "tr = pd.read_csv(base_url + \"train.csv\")\n",
        "tex = pd.read_csv(base_url + \"text.csv\")\n",
        "reference = pd.read_csv(base_url + \"reference.csv\", names=[\"target\", \"source\"], skiprows=1)\n",
        "t = pd.read_csv(base_url + \"test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekSgznyucz-0",
        "colab_type": "text"
      },
      "source": [
        "# Stellargraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXAJupGwmU6Y",
        "colab_type": "text"
      },
      "source": [
        "## Imports & Installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGlaQatm-x6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install stellargraph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3sih_z2_pZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import networkx as nx\n",
        "\n",
        "\n",
        "# StellarGraph\n",
        "import stellargraph as sg\n",
        "from stellargraph.mapper import FullBatchNodeGenerator, GraphSAGENodeGenerator\n",
        "from stellargraph.layer import GCN, GraphSAGE, GAT\n",
        "\n",
        "# Text libraries\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import model_selection\n",
        "\n",
        "\n",
        "# Keras\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras import layers, optimizers, losses, metrics, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Input\n",
        "from tensorflow.keras.models import Sequential\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyPGhL4-z2ax",
        "colab_type": "text"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Leh6rawrHg2i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Joining dataframes\n",
        "\n",
        "data = pd.merge(tr, tex, left_on='id', right_on='id', how='left')\n",
        "test = pd.merge(t, tex, left_on='id', right_on='id', how='left')\n",
        "\n",
        "# Building vocab\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(tex.title)\n",
        "\n",
        "# Transform the data and test DF\n",
        "\n",
        "bowd = vectorizer.transform(data.title).toarray()\n",
        "bowt = vectorizer.transform(test.title).toarray()\n",
        "\n",
        "# Concatenating BoW with original DF\n",
        "\n",
        "data = pd.concat([tr, pd.DataFrame(bowd)], axis=1)\n",
        "data.set_index('id', inplace=True)\n",
        "test = pd.concat([t, pd.DataFrame(bowt)], axis=1)\n",
        "test.set_index('id', inplace=True)\n",
        "\n",
        "# Creating one graph\n",
        "\n",
        "df_all = pd.concat([data,test]).drop('label', axis=1)\n",
        "df_all_lab = pd.concat([data,test])\n",
        "\n",
        "# Train / test mask\n",
        "\n",
        "train_mask = (np.isnan(df_all_lab['label']) == False)\n",
        "test_mask = (np.isnan(df_all_lab['label'] == True)) \n",
        "\n",
        "# Labels to one-hot\n",
        "\n",
        "Y_train_all = to_categorical(data['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhHbaRxXNmyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting in train and valid\n",
        "\n",
        "train_data, valid_data = model_selection.train_test_split(data, train_size=0.95)\n",
        "\n",
        "# Creating labels\n",
        "\n",
        "Y_train = to_categorical(train_data['label'])\n",
        "Y_valid = to_categorical(valid_data['label'])\n",
        "\n",
        "# Features\n",
        "\n",
        "X_train = train_data.drop('label', axis=1)\n",
        "X_valid = valid_data.drop('label', axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPDKaXVfCd-2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating graph\n",
        "\n",
        "graph = sg.StellarDiGraph(nodes={\"paper\": df_all}, edges={\"cites\": reference})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC83eNTGCetF",
        "colab_type": "text"
      },
      "source": [
        "## GCN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-T36owvvYze",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5eae86ec-e176-41b0-ce20-6bc63646a328"
      },
      "source": [
        "generator = FullBatchNodeGenerator(graph, method=\"gcn\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using GCN (local pooling) filters...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVvP1DV_wY30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_gen = generator.flow(X_train.index, Y_train)\n",
        "val_gen = generator.flow(X_valid.index, Y_valid)\n",
        "\n",
        "# Building model\n",
        "gcn = GCN(layer_sizes=[128,128], activations=[\"relu\", \"relu\"], generator=generator, dropout=0.3)\n",
        "x_in, x_out = gcn.build()\n",
        "predictions = layers.Dense(units=Y_train.shape[1], activation='softmax')(x_out)\n",
        "\n",
        "modelGCN = Model(inputs=x_in, outputs=predictions)\n",
        "modelGCN.compile(optimizer=optimizers.Adam(lr=0.001),\n",
        "                 loss='categorical_crossentropy',\n",
        "                 metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMtVtVLtxaT9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "if not os.path.isdir(\"logs\"):\n",
        "    os.makedirs(\"logs\")\n",
        "es_callback = EarlyStopping(\n",
        "    monitor=\"val_acc\", patience=50\n",
        ")  # patience is the number of epochs to wait before early stopping in case of no further improvement\n",
        "mc_callback = ModelCheckpoint(\n",
        "    \"logs/best_model.h5\", monitor=\"val_acc\", save_best_only=True, save_weights_only=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efzPdZ-txc74",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b6ac3956-5064-4b22-f9c1-a94840466e36"
      },
      "source": [
        "historyGCN = modelGCN.fit_generator(\n",
        "    train_gen,\n",
        "    epochs=200,\n",
        "    validation_data=val_gen,\n",
        "    verbose=2,\n",
        "    shuffle=False,  # this should be False, since shuffling data means shuffling the whole graph\n",
        "    callbacks=[es_callback, mc_callback],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-12-efa1bfdfacec>:7: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/200\n",
            "1/1 - 3s - loss: 1.6087 - acc: 0.2132 - val_loss: 1.6009 - val_acc: 0.4366\n",
            "Epoch 2/200\n",
            "1/1 - 3s - loss: 1.5999 - acc: 0.3942 - val_loss: 1.5926 - val_acc: 0.5133\n",
            "Epoch 3/200\n",
            "1/1 - 3s - loss: 1.5904 - acc: 0.4851 - val_loss: 1.5827 - val_acc: 0.5790\n",
            "Epoch 4/200\n",
            "1/1 - 3s - loss: 1.5795 - acc: 0.5499 - val_loss: 1.5706 - val_acc: 0.6072\n",
            "Epoch 5/200\n",
            "1/1 - 3s - loss: 1.5666 - acc: 0.5792 - val_loss: 1.5564 - val_acc: 0.6213\n",
            "Epoch 6/200\n",
            "1/1 - 3s - loss: 1.5515 - acc: 0.5984 - val_loss: 1.5399 - val_acc: 0.6322\n",
            "Epoch 7/200\n",
            "1/1 - 3s - loss: 1.5342 - acc: 0.6119 - val_loss: 1.5212 - val_acc: 0.6385\n",
            "Epoch 8/200\n",
            "1/1 - 3s - loss: 1.5140 - acc: 0.6269 - val_loss: 1.5003 - val_acc: 0.6479\n",
            "Epoch 9/200\n",
            "1/1 - 3s - loss: 1.4924 - acc: 0.6360 - val_loss: 1.4773 - val_acc: 0.6573\n",
            "Epoch 10/200\n",
            "1/1 - 2s - loss: 1.4680 - acc: 0.6441 - val_loss: 1.4521 - val_acc: 0.6620\n",
            "Epoch 11/200\n",
            "1/1 - 2s - loss: 1.4418 - acc: 0.6509 - val_loss: 1.4249 - val_acc: 0.6729\n",
            "Epoch 12/200\n",
            "1/1 - 2s - loss: 1.4122 - acc: 0.6593 - val_loss: 1.3956 - val_acc: 0.6823\n",
            "Epoch 13/200\n",
            "1/1 - 2s - loss: 1.3810 - acc: 0.6739 - val_loss: 1.3642 - val_acc: 0.6886\n",
            "Epoch 14/200\n",
            "1/1 - 2s - loss: 1.3494 - acc: 0.6868 - val_loss: 1.3309 - val_acc: 0.6933\n",
            "Epoch 15/200\n",
            "1/1 - 2s - loss: 1.3136 - acc: 0.6989 - val_loss: 1.2957 - val_acc: 0.6917\n",
            "Epoch 16/200\n",
            "1/1 - 2s - loss: 1.2779 - acc: 0.7130 - val_loss: 1.2587 - val_acc: 0.7089\n",
            "Epoch 17/200\n",
            "1/1 - 3s - loss: 1.2391 - acc: 0.7240 - val_loss: 1.2201 - val_acc: 0.7230\n",
            "Epoch 18/200\n",
            "1/1 - 2s - loss: 1.1999 - acc: 0.7388 - val_loss: 1.1801 - val_acc: 0.7293\n",
            "Epoch 19/200\n",
            "1/1 - 2s - loss: 1.1582 - acc: 0.7552 - val_loss: 1.1391 - val_acc: 0.7449\n",
            "Epoch 20/200\n",
            "1/1 - 2s - loss: 1.1180 - acc: 0.7627 - val_loss: 1.0973 - val_acc: 0.7653\n",
            "Epoch 21/200\n",
            "1/1 - 2s - loss: 1.0742 - acc: 0.7787 - val_loss: 1.0552 - val_acc: 0.7762\n",
            "Epoch 22/200\n",
            "1/1 - 2s - loss: 1.0311 - acc: 0.7928 - val_loss: 1.0130 - val_acc: 0.7919\n",
            "Epoch 23/200\n",
            "1/1 - 3s - loss: 0.9878 - acc: 0.7977 - val_loss: 0.9712 - val_acc: 0.7997\n",
            "Epoch 24/200\n",
            "1/1 - 3s - loss: 0.9443 - acc: 0.8074 - val_loss: 0.9301 - val_acc: 0.8013\n",
            "Epoch 25/200\n",
            "1/1 - 3s - loss: 0.9033 - acc: 0.8174 - val_loss: 0.8901 - val_acc: 0.8044\n",
            "Epoch 26/200\n",
            "1/1 - 3s - loss: 0.8603 - acc: 0.8211 - val_loss: 0.8515 - val_acc: 0.8106\n",
            "Epoch 27/200\n",
            "1/1 - 2s - loss: 0.8187 - acc: 0.8294 - val_loss: 0.8146 - val_acc: 0.8138\n",
            "Epoch 28/200\n",
            "1/1 - 3s - loss: 0.7835 - acc: 0.8316 - val_loss: 0.7795 - val_acc: 0.8200\n",
            "Epoch 29/200\n",
            "1/1 - 3s - loss: 0.7418 - acc: 0.8355 - val_loss: 0.7466 - val_acc: 0.8216\n",
            "Epoch 30/200\n",
            "1/1 - 2s - loss: 0.7117 - acc: 0.8372 - val_loss: 0.7158 - val_acc: 0.8232\n",
            "Epoch 31/200\n",
            "1/1 - 2s - loss: 0.6732 - acc: 0.8418 - val_loss: 0.6874 - val_acc: 0.8232\n",
            "Epoch 32/200\n",
            "1/1 - 2s - loss: 0.6412 - acc: 0.8480 - val_loss: 0.6613 - val_acc: 0.8232\n",
            "Epoch 33/200\n",
            "1/1 - 2s - loss: 0.6117 - acc: 0.8466 - val_loss: 0.6376 - val_acc: 0.8200\n",
            "Epoch 34/200\n",
            "1/1 - 2s - loss: 0.5888 - acc: 0.8483 - val_loss: 0.6161 - val_acc: 0.8232\n",
            "Epoch 35/200\n",
            "1/1 - 2s - loss: 0.5628 - acc: 0.8524 - val_loss: 0.5969 - val_acc: 0.8279\n",
            "Epoch 36/200\n",
            "1/1 - 2s - loss: 0.5380 - acc: 0.8540 - val_loss: 0.5799 - val_acc: 0.8279\n",
            "Epoch 37/200\n",
            "1/1 - 3s - loss: 0.5146 - acc: 0.8578 - val_loss: 0.5651 - val_acc: 0.8326\n",
            "Epoch 38/200\n",
            "1/1 - 2s - loss: 0.4989 - acc: 0.8587 - val_loss: 0.5521 - val_acc: 0.8357\n",
            "Epoch 39/200\n",
            "1/1 - 2s - loss: 0.4834 - acc: 0.8580 - val_loss: 0.5409 - val_acc: 0.8326\n",
            "Epoch 40/200\n",
            "1/1 - 2s - loss: 0.4701 - acc: 0.8614 - val_loss: 0.5313 - val_acc: 0.8294\n",
            "Epoch 41/200\n",
            "1/1 - 3s - loss: 0.4513 - acc: 0.8636 - val_loss: 0.5230 - val_acc: 0.8294\n",
            "Epoch 42/200\n",
            "1/1 - 3s - loss: 0.4379 - acc: 0.8628 - val_loss: 0.5159 - val_acc: 0.8294\n",
            "Epoch 43/200\n",
            "1/1 - 3s - loss: 0.4310 - acc: 0.8639 - val_loss: 0.5098 - val_acc: 0.8294\n",
            "Epoch 44/200\n",
            "1/1 - 3s - loss: 0.4213 - acc: 0.8671 - val_loss: 0.5047 - val_acc: 0.8310\n",
            "Epoch 45/200\n",
            "1/1 - 2s - loss: 0.4100 - acc: 0.8671 - val_loss: 0.5003 - val_acc: 0.8341\n",
            "Epoch 46/200\n",
            "1/1 - 2s - loss: 0.4015 - acc: 0.8689 - val_loss: 0.4965 - val_acc: 0.8341\n",
            "Epoch 47/200\n",
            "1/1 - 2s - loss: 0.3965 - acc: 0.8708 - val_loss: 0.4933 - val_acc: 0.8388\n",
            "Epoch 48/200\n",
            "1/1 - 2s - loss: 0.3824 - acc: 0.8725 - val_loss: 0.4907 - val_acc: 0.8388\n",
            "Epoch 49/200\n",
            "1/1 - 2s - loss: 0.3807 - acc: 0.8704 - val_loss: 0.4886 - val_acc: 0.8388\n",
            "Epoch 50/200\n",
            "1/1 - 3s - loss: 0.3770 - acc: 0.8734 - val_loss: 0.4869 - val_acc: 0.8372\n",
            "Epoch 51/200\n",
            "1/1 - 2s - loss: 0.3702 - acc: 0.8754 - val_loss: 0.4856 - val_acc: 0.8404\n",
            "Epoch 52/200\n",
            "1/1 - 2s - loss: 0.3609 - acc: 0.8758 - val_loss: 0.4844 - val_acc: 0.8404\n",
            "Epoch 53/200\n",
            "1/1 - 3s - loss: 0.3615 - acc: 0.8751 - val_loss: 0.4834 - val_acc: 0.8388\n",
            "Epoch 54/200\n",
            "1/1 - 2s - loss: 0.3585 - acc: 0.8767 - val_loss: 0.4826 - val_acc: 0.8404\n",
            "Epoch 55/200\n",
            "1/1 - 2s - loss: 0.3429 - acc: 0.8794 - val_loss: 0.4822 - val_acc: 0.8404\n",
            "Epoch 56/200\n",
            "1/1 - 2s - loss: 0.3426 - acc: 0.8801 - val_loss: 0.4819 - val_acc: 0.8419\n",
            "Epoch 57/200\n",
            "1/1 - 2s - loss: 0.3384 - acc: 0.8839 - val_loss: 0.4815 - val_acc: 0.8419\n",
            "Epoch 58/200\n",
            "1/1 - 2s - loss: 0.3352 - acc: 0.8829 - val_loss: 0.4814 - val_acc: 0.8404\n",
            "Epoch 59/200\n",
            "1/1 - 3s - loss: 0.3344 - acc: 0.8831 - val_loss: 0.4815 - val_acc: 0.8451\n",
            "Epoch 60/200\n",
            "1/1 - 2s - loss: 0.3303 - acc: 0.8853 - val_loss: 0.4817 - val_acc: 0.8466\n",
            "Epoch 61/200\n",
            "1/1 - 2s - loss: 0.3271 - acc: 0.8853 - val_loss: 0.4820 - val_acc: 0.8466\n",
            "Epoch 62/200\n",
            "1/1 - 2s - loss: 0.3168 - acc: 0.8894 - val_loss: 0.4825 - val_acc: 0.8404\n",
            "Epoch 63/200\n",
            "1/1 - 2s - loss: 0.3166 - acc: 0.8894 - val_loss: 0.4831 - val_acc: 0.8404\n",
            "Epoch 64/200\n",
            "1/1 - 2s - loss: 0.3139 - acc: 0.8908 - val_loss: 0.4837 - val_acc: 0.8388\n",
            "Epoch 65/200\n",
            "1/1 - 2s - loss: 0.3148 - acc: 0.8915 - val_loss: 0.4842 - val_acc: 0.8388\n",
            "Epoch 66/200\n",
            "1/1 - 2s - loss: 0.3061 - acc: 0.8933 - val_loss: 0.4848 - val_acc: 0.8388\n",
            "Epoch 67/200\n",
            "1/1 - 2s - loss: 0.3021 - acc: 0.8964 - val_loss: 0.4855 - val_acc: 0.8388\n",
            "Epoch 68/200\n",
            "1/1 - 2s - loss: 0.3023 - acc: 0.8914 - val_loss: 0.4863 - val_acc: 0.8357\n",
            "Epoch 69/200\n",
            "1/1 - 2s - loss: 0.3003 - acc: 0.8952 - val_loss: 0.4872 - val_acc: 0.8372\n",
            "Epoch 70/200\n",
            "1/1 - 2s - loss: 0.2955 - acc: 0.8965 - val_loss: 0.4884 - val_acc: 0.8372\n",
            "Epoch 71/200\n",
            "1/1 - 2s - loss: 0.2944 - acc: 0.8976 - val_loss: 0.4896 - val_acc: 0.8341\n",
            "Epoch 72/200\n",
            "1/1 - 3s - loss: 0.2942 - acc: 0.8970 - val_loss: 0.4910 - val_acc: 0.8310\n",
            "Epoch 73/200\n",
            "1/1 - 2s - loss: 0.2871 - acc: 0.8995 - val_loss: 0.4928 - val_acc: 0.8279\n",
            "Epoch 74/200\n",
            "1/1 - 3s - loss: 0.2887 - acc: 0.8988 - val_loss: 0.4945 - val_acc: 0.8294\n",
            "Epoch 75/200\n",
            "1/1 - 2s - loss: 0.2847 - acc: 0.8992 - val_loss: 0.4963 - val_acc: 0.8263\n",
            "Epoch 76/200\n",
            "1/1 - 2s - loss: 0.2794 - acc: 0.9015 - val_loss: 0.4977 - val_acc: 0.8279\n",
            "Epoch 77/200\n",
            "1/1 - 2s - loss: 0.2771 - acc: 0.9044 - val_loss: 0.4991 - val_acc: 0.8294\n",
            "Epoch 78/200\n",
            "1/1 - 2s - loss: 0.2804 - acc: 0.9009 - val_loss: 0.5008 - val_acc: 0.8263\n",
            "Epoch 79/200\n",
            "1/1 - 2s - loss: 0.2743 - acc: 0.9024 - val_loss: 0.5020 - val_acc: 0.8232\n",
            "Epoch 80/200\n",
            "1/1 - 2s - loss: 0.2695 - acc: 0.9055 - val_loss: 0.5026 - val_acc: 0.8232\n",
            "Epoch 81/200\n",
            "1/1 - 2s - loss: 0.2704 - acc: 0.9056 - val_loss: 0.5033 - val_acc: 0.8247\n",
            "Epoch 82/200\n",
            "1/1 - 2s - loss: 0.2662 - acc: 0.9071 - val_loss: 0.5043 - val_acc: 0.8247\n",
            "Epoch 83/200\n",
            "1/1 - 2s - loss: 0.2709 - acc: 0.9044 - val_loss: 0.5050 - val_acc: 0.8216\n",
            "Epoch 84/200\n",
            "1/1 - 2s - loss: 0.2642 - acc: 0.9052 - val_loss: 0.5058 - val_acc: 0.8216\n",
            "Epoch 85/200\n",
            "1/1 - 2s - loss: 0.2685 - acc: 0.9044 - val_loss: 0.5064 - val_acc: 0.8232\n",
            "Epoch 86/200\n",
            "1/1 - 2s - loss: 0.2611 - acc: 0.9064 - val_loss: 0.5070 - val_acc: 0.8232\n",
            "Epoch 87/200\n",
            "1/1 - 3s - loss: 0.2641 - acc: 0.9058 - val_loss: 0.5077 - val_acc: 0.8232\n",
            "Epoch 88/200\n",
            "1/1 - 2s - loss: 0.2582 - acc: 0.9070 - val_loss: 0.5085 - val_acc: 0.8216\n",
            "Epoch 89/200\n",
            "1/1 - 2s - loss: 0.2616 - acc: 0.9081 - val_loss: 0.5095 - val_acc: 0.8216\n",
            "Epoch 90/200\n",
            "1/1 - 2s - loss: 0.2489 - acc: 0.9109 - val_loss: 0.5105 - val_acc: 0.8216\n",
            "Epoch 91/200\n",
            "1/1 - 2s - loss: 0.2537 - acc: 0.9105 - val_loss: 0.5115 - val_acc: 0.8216\n",
            "Epoch 92/200\n",
            "1/1 - 2s - loss: 0.2542 - acc: 0.9097 - val_loss: 0.5125 - val_acc: 0.8232\n",
            "Epoch 93/200\n",
            "1/1 - 2s - loss: 0.2550 - acc: 0.9079 - val_loss: 0.5137 - val_acc: 0.8232\n",
            "Epoch 94/200\n",
            "1/1 - 2s - loss: 0.2504 - acc: 0.9120 - val_loss: 0.5150 - val_acc: 0.8216\n",
            "Epoch 95/200\n",
            "1/1 - 2s - loss: 0.2458 - acc: 0.9111 - val_loss: 0.5168 - val_acc: 0.8200\n",
            "Epoch 96/200\n",
            "1/1 - 2s - loss: 0.2493 - acc: 0.9100 - val_loss: 0.5181 - val_acc: 0.8200\n",
            "Epoch 97/200\n",
            "1/1 - 2s - loss: 0.2428 - acc: 0.9122 - val_loss: 0.5190 - val_acc: 0.8200\n",
            "Epoch 98/200\n",
            "1/1 - 2s - loss: 0.2469 - acc: 0.9109 - val_loss: 0.5197 - val_acc: 0.8216\n",
            "Epoch 99/200\n",
            "1/1 - 2s - loss: 0.2443 - acc: 0.9107 - val_loss: 0.5202 - val_acc: 0.8216\n",
            "Epoch 100/200\n",
            "1/1 - 2s - loss: 0.2372 - acc: 0.9159 - val_loss: 0.5211 - val_acc: 0.8200\n",
            "Epoch 101/200\n",
            "1/1 - 2s - loss: 0.2430 - acc: 0.9115 - val_loss: 0.5221 - val_acc: 0.8185\n",
            "Epoch 102/200\n",
            "1/1 - 2s - loss: 0.2403 - acc: 0.9133 - val_loss: 0.5235 - val_acc: 0.8200\n",
            "Epoch 103/200\n",
            "1/1 - 2s - loss: 0.2387 - acc: 0.9136 - val_loss: 0.5249 - val_acc: 0.8216\n",
            "Epoch 104/200\n",
            "1/1 - 2s - loss: 0.2313 - acc: 0.9185 - val_loss: 0.5264 - val_acc: 0.8216\n",
            "Epoch 105/200\n",
            "1/1 - 2s - loss: 0.2313 - acc: 0.9161 - val_loss: 0.5275 - val_acc: 0.8200\n",
            "Epoch 106/200\n",
            "1/1 - 2s - loss: 0.2331 - acc: 0.9152 - val_loss: 0.5288 - val_acc: 0.8216\n",
            "Epoch 107/200\n",
            "1/1 - 2s - loss: 0.2344 - acc: 0.9156 - val_loss: 0.5304 - val_acc: 0.8216\n",
            "Epoch 108/200\n",
            "1/1 - 2s - loss: 0.2254 - acc: 0.9174 - val_loss: 0.5320 - val_acc: 0.8216\n",
            "Epoch 109/200\n",
            "1/1 - 2s - loss: 0.2314 - acc: 0.9170 - val_loss: 0.5334 - val_acc: 0.8216\n",
            "Epoch 110/200\n",
            "1/1 - 2s - loss: 0.2310 - acc: 0.9137 - val_loss: 0.5346 - val_acc: 0.8216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTcnio-frEV5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b057274d-712e-457d-f9b4-68f819893ad2"
      },
      "source": [
        "# Predictions based on \"test\" set\n",
        "\n",
        "test_gen_gcn = generator.flow(test.index)\n",
        "modelGCN.load_weights(\"logs/best_model.h5\")\n",
        "pred_gcn = modelGCN.predict_generator(test_gen_gcn)\n",
        "\n",
        "final_pred_gcn = np.argmax(pred_gcn.squeeze(), axis=1)\n",
        "\n",
        "pred_out_gcn = test\n",
        "pred_out_gcn['label'] = final_pred_gcn\n",
        "pred_out_gcn = pred_out_gcn.loc[:,['label']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-13-30928ef1aba8>:3: Model.predict_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.predict, which supports generators.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14O3JzltDIRX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Exporting\n",
        "\n",
        "pred_out_gcn.to_csv('pred_gcn2.csv')\n",
        "!cp pred_gcn2.csv \"drive/My Drive/deep learning/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ik0Ic8_bz7Pb",
        "colab_type": "text"
      },
      "source": [
        "## GraphSage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucj1lSm6z_F4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13bae6d2-4fd7-4105-8bce-764510e047b8"
      },
      "source": [
        "batch_size = 50\n",
        "num_samples = [10,5]\n",
        "\n",
        "generatorSAGE = GraphSAGENodeGenerator(graph, batch_size, num_samples)\n",
        "\n",
        "trainSAGE_gen = generatorSAGE.flow(X_train.index, Y_train, shuffle=True)\n",
        "\n",
        "graphsage_model = GraphSAGE(layer_sizes=[32,32], generator=generatorSAGE, bias=True, dropout=0.5)\n",
        "\n",
        "xs_in, xs_out = graphsage_model.build()\n",
        "predictionSAGE = layers.Dense(units=Y_train.shape[1], activation='softmax')(xs_out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: The 'build' method is deprecated, use 'in_out_tensors' instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENpIgLSK7Wac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modelSAGE = Model(xs_in, predictionSAGE)\n",
        "modelSAGE.compile(optimizer=optimizers.Adam(lr=0.001), loss='categorical_crossentropy', metrics=['acc'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YInnKrdH7ukm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.isdir(\"logs\"):\n",
        "    os.makedirs(\"logs\")\n",
        "es_callback = EarlyStopping(\n",
        "    monitor=\"val_acc\", patience=20\n",
        ")  # patience is the number of epochs to wait before early stopping in case of no further improvement\n",
        "mc_callback = ModelCheckpoint(\n",
        "    \"logs/best_model.h5\", monitor=\"val_acc\", save_best_only=True, save_weights_only=True\n",
        ")\n",
        "\n",
        "validSAGE_gen = generatorSAGE.flow(X_valid.index, Y_valid)\n",
        "historySAGE = modelSAGE.fit_generator(trainSAGE_gen, epochs=75, validation_data=validSAGE_gen, verbose=2, shuffle=False, callbacks=[es_callback, mc_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxYrV6vPndiP",
        "colab_type": "text"
      },
      "source": [
        "## GAT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgVjqVwcnf7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generatorGAT = FullBatchNodeGenerator(graph, method=\"gat\")\n",
        "\n",
        "train_gat = generatorGAT.flow(X_train.index, Y_train)\n",
        "val_gat =  generatorGAT.flow(X_valid.index, Y_valid)\n",
        "\n",
        "gat = GAT(\n",
        "    layer_sizes=[256,256,256, Y_train.shape[1]],\n",
        "    activations=[\"elu\", \"elu\", \"elu\", \"softmax\"],\n",
        "    attn_heads=6,\n",
        "    generator=generatorGAT,\n",
        "    in_dropout=0.4,\n",
        "    attn_dropout=0.5,\n",
        "    normalize=None,\n",
        ")\n",
        "\n",
        "xga_in, xga_pred = gat.in_out_tensors()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6waASoN3ocJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modelGAT = Model(inputs=xga_in, outputs=xga_pred)\n",
        "modelGAT.compile(optimizer=optimizers.Adam(lr=0.005),\n",
        "                 loss=losses.categorical_crossentropy,\n",
        "                 metrics=['acc'],)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b27bFOVDpEhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "historyGAT = modelGAT.fit(train_gat, epochs=150, validation_data=val_gat, verbose=2, shuffle=False, callbacks=[es_callback, mc_callback],)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVuVDA2jfdwe",
        "colab_type": "text"
      },
      "source": [
        "# Pytorch Geometric\n",
        "\n",
        "For some reason, I could not get pytorch geometric to obtain results comparable to Stellargraph using the same models. This is too bad as it is much faster than Stellargraph, and SplineCNN showed great promise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOV8uPvgmRRt",
        "colab_type": "text"
      },
      "source": [
        "## Imports & Installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmhSoCy5fgyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
        "!pip install torch-sparse==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
        "!pip install torch-spline-conv==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
        "!pip install torch-geometric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eACYvoH0fiFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import SplineConv, GATConv\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from torch_geometric.utils import degree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ga3hZMqvifpl",
        "colab_type": "text"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cPyO-z_fwCg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adding edge weight column (some models require edge attributes)\n",
        "\n",
        "reference['weight'] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2tolS3zf1NS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Joining dataframes\n",
        "\n",
        "data = pd.merge(tr, tex, left_on='id', right_on='id', how='left')\n",
        "test_pd = pd.merge(t, tex, left_on='id', right_on='id', how='left')\n",
        "\n",
        "# Building vocab\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(tex.title)\n",
        "\n",
        "# Transform the data and test DF\n",
        "\n",
        "bowd = vectorizer.transform(data.title).toarray()\n",
        "bowt = vectorizer.transform(test_pd.title).toarray()\n",
        "\n",
        "# Concatenating BoW with original DF\n",
        "\n",
        "data = pd.concat([tr, pd.DataFrame(bowd)], axis=1)\n",
        "data.set_index('id', inplace=True)\n",
        "test_pd = pd.concat([t, pd.DataFrame(bowt)], axis=1)\n",
        "test_pd.set_index('id', inplace=True)\n",
        "\n",
        "# Creating one DF\n",
        "df_all = pd.concat([data,test_pd]).drop('label', axis=1)\n",
        "df_all_lab = pd.concat([data,test_pd])\n",
        "\n",
        "# Creating masks\n",
        "\"\"\"\n",
        "This is my crude way of creating masks when we have unknowns (data to predict).\n",
        "If you could provide a cleaner way for me to do so, I would be very happy, because this is barely acceptable, lol.\n",
        "Although now that I am writing this, I am thinking that we can simply leave out the \"test\" set and still provide the full\n",
        "graph to the model... \n",
        "\"\"\"\n",
        "# 80/20 valid split\n",
        "df_all_lab['randNumCol'] = np.random.randint(1, 21, df_all_lab.shape[0])\n",
        "\n",
        "test_maskdl = torch.tensor((np.isnan(df_all_lab['label']) == True), dtype=torch.bool)\n",
        "train_maskdl = torch.tensor((np.isnan(df_all_lab['label']) == False) & (df_all_lab.randNumCol < 19), dtype=torch.bool)\n",
        "valid_maskdl = torch.tensor((np.isnan(df_all_lab['label']) == False) & (df_all_lab.randNumCol == 20), dtype=torch.bool)\n",
        "df_all_lab = df_all_lab.drop(['randNumCol'], axis=1)\n",
        "\n",
        "\n",
        "# Since we cannot have NA's, I replaced by 5 (which will not be used at output will be 5 labels)\n",
        "df_all_lab['label'].fillna(5, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR5fWMLwg6MK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting to torch tensor\n",
        "\n",
        "edg_temp = torch.tensor(reference['weight'].values, dtype=torch.float)\n",
        "edge_attrdl = edg_temp.unsqueeze(1)\n",
        "edge_indexdl = torch.tensor(reference.drop('weight', axis=1).values, dtype=torch.long)\n",
        "xdl = torch.tensor(df_all_lab.drop('label',axis=1).values, dtype=torch.float)\n",
        "ydl = torch.tensor(df_all_lab['label'].values, dtype=torch.long)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SO8FNwP1g8aA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute normalized degree (not necessary)\n",
        "te = T.TargetIndegree()\n",
        "\n",
        "# Creating data for pytorch geo\n",
        "temp = Data(x=xdl, edge_index=edge_indexdl.t().contiguous(), y=ydl)\n",
        "\n",
        "data_dl = te(temp)\n",
        "\n",
        "# Loading masks in data\n",
        "data_dl.train_mask = train_maskdl\n",
        "data_dl.valid_mask = valid_maskdl\n",
        "data_dl.test_mask = test_maskdl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhSG-n6ahd2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model independent \n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    F.nll_loss(model()[data_dl.train_mask], data_dl.y[data_dl.train_mask]).backward()\n",
        "    optimizer.step()\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    logits, accs = model(), []\n",
        "    for _, mask in data_dl('train_mask', 'valid_mask'):\n",
        "        pred = logits[mask].max(1)[1]\n",
        "        acc = pred.eq(data_dl.y[mask]).sum().item() / mask.sum().item()\n",
        "        accs.append(acc)\n",
        "    return accs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-vXomonhW71",
        "colab_type": "text"
      },
      "source": [
        "## SplineCNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUVTqOtBhWB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining model\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = SplineConv(data_dl.num_features, 128, dim=1, kernel_size=2)\n",
        "        self.conv2 = SplineConv(128, 5, dim=1, kernel_size=2)\n",
        "\n",
        "    def forward(self):\n",
        "        x, edge_index, edge_attr = data_dl.x, data_dl.edge_index, data_dl.edge_attr\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = F.elu(self.conv1(x, edge_index, edge_attr))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index, edge_attr)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqV9UqZEh09j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model, data = Net().to(device), data_dl.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQkWyN0qh_Be",
        "colab_type": "text"
      },
      "source": [
        "## GAT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ScIMJ1ah_tU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining model\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = GATConv(data_dl.num_features, 256, heads=6), dropout=0.6)\n",
        "        self.conv2 = GATConv(\n",
        "            256 * 6, 5, heads=1, concat=True, dropout=0.6)\n",
        "        \n",
        "    def forward(self):\n",
        "        x = F.dropout(data_dl.x, p=0.6, training=self.training)\n",
        "        x = F.elu(self.conv1(x, data_dl.edge_index))\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = self.conv2(x, data_dl.edge_index)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcDTlPbZiQa6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model, data = Net().to(device), data_dl.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5pjmKA-inUM",
        "colab_type": "text"
      },
      "source": [
        "## Training & Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZqKSSTlh-R0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training\n",
        "\n",
        "for epoch in range(1, 16):\n",
        "    train()\n",
        "    log = 'Epoch: {:03d}, Train: {:.4f}, Test: {:.4f}'\n",
        "    print(log.format(epoch, *test()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1EM38sWiA6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predictions\n",
        "\n",
        "pred = model()[data_dl.test_mask]\n",
        "final_pred = np.argmax(pred.data.cpu(), axis=1).numpy()\n",
        "\n",
        "pred_out_gcn = test_pd\n",
        "pred_out_gcn['label'] = final_pred\n",
        "pred_out_gcn = pred_out_gcn.loc[:,['label']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTsIjkRiiyqn",
        "colab_type": "text"
      },
      "source": [
        "# Spektral\n",
        "\n",
        "This is the first library I tried. I strongly prefer the other two."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNrZfvsAmNoG",
        "colab_type": "text"
      },
      "source": [
        "## Imports & Installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpyr0z4gjASq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Installs\n",
        "\n",
        "! apt install graphviz libgraphviz-dev libcgraph6\n",
        "! pip install spektral"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWYPOIqpjHTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "\n",
        "import math\n",
        "import numpy.ma as ma\n",
        "\n",
        "# Graph reprensentations\n",
        "from spektral.layers import GraphConv, ChebConv, ARMAConv, GraphAttention\n",
        "from spektral import utils\n",
        "from spektral.utils.misc import add_eye\n",
        "\n",
        "# Keras\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Input, Dropout, Embedding, LSTM, SpatialDropout1D, Dense, Flatten\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# Utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy import sparse\n",
        "import networkx as nx\n",
        "\n",
        "# Language processing\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "from nltk import word_tokenize\n",
        "\n",
        "import random\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B6py36HjtO-",
        "colab_type": "text"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1Vult8qjRp2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removing stopwords \n",
        "\n",
        "text['title'] = text['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
        "\n",
        "# Getting maximum length for padding\n",
        "\n",
        "max_seq_length = text['title'].map(lambda x: len(x.split())).max()\n",
        "\n",
        "# Building vocab\n",
        "\n",
        "tokenizer = Tokenizer(lower=True)\n",
        "tokenizer.fit_on_texts(text['title'].values)\n",
        "\n",
        "# Get number of words\n",
        "\n",
        "num_words = len(tokenizer.word_index)+1\n",
        "\n",
        "train_df = pd.merge(tr, text, left_on='id', right_on='id', how='left')\n",
        "test_df = pd.merge(te, text, left_on='id', right_on='id', how='left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7bV87Rzj5WS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preparing data for Spektral.\n",
        "# Note that at this point, I was a bit overwhelmed, resulting in weird codes\n",
        "\n",
        "full_data = pd.concat([train_df, test_df])\n",
        "full_data = full_data.sort_values(by=['id']).reset_index()\n",
        "full_data = full_data.drop(['index', 'id'], axis=1)\n",
        "full_data['label'].fillna(-1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uPNaBDVkTXn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating masks\n",
        "\n",
        "# 80/20 valid split\n",
        "\"\"\"\n",
        "This method of splitting was also used in Pyotrch Geometric. Once again, let me know what\n",
        "you would have done instead!\n",
        "\"\"\"\n",
        "full_data['randNumCol'] = np.random.randint(1, 6, full_data.shape[0])\n",
        "\n",
        "test_mask = (full_data['label'] == -1)\n",
        "train_mask = (full_data['label'] != -1) & (full_data.randNumCol < 5)\n",
        "valid_mask = (full_data['label'] != -1) & (full_data.randNumCol == 5)    \n",
        "\n",
        "full_data = full_data.drop(['randNumCol'], axis=1)\n",
        "\n",
        "# To one-hot\n",
        "Y_g = pd.get_dummies(full_data['label'].values).drop(-1, axis=1).to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6z4eeV2kh_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating sparse adjacency matrix\n",
        "\n",
        "size = len(text)\n",
        "adj_matrix = [[0]*size for _ in range(size)]\n",
        "\n",
        "for i, id in reference.values.tolist():\n",
        "  adj_matrix[i][id] = 1\n",
        "\n",
        "adj = sparse.csr_matrix(np.asarray(adj_matrix))\n",
        "print(adj.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSOHOMp7kpc-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenizing\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(full_data['title'].values)\n",
        "\n",
        "X_tok = tokenizer.texts_to_sequences(full_data['title'].values)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index)+1\n",
        "X_tok = pad_sequences(X_tok, padding='post', maxlen=max_seq_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0koG3Gskki4",
        "colab_type": "text"
      },
      "source": [
        "## GCN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXCDBVw0kjoq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocess\n",
        "\n",
        "A_filter = utils.localpooling_filter(adj).astype('f4')\n",
        "\n",
        "# Model's Definitions\n",
        "N = adj.shape[0]\n",
        "F = X_tok.shape[-1]\n",
        "n_class = Y_g.shape[-1]\n",
        "dropout = 0.4\n",
        "lr = 0.001\n",
        "epochs = 1000\n",
        "patience = 50\n",
        "regularizer = 0.00005\n",
        "\n",
        "# Building model\n",
        "X_input = Input(shape=(F, ))\n",
        "A_input = Input((N, ), sparse=True)\n",
        "\n",
        "dropout_1 = Dropout(0.5)(X_input)\n",
        "graph_conv1 = GraphConv(36, activation='relu', use_bias=False, kernel_regularizer=l2(regularizer))([dropout_1, A_input])\n",
        "dropout_2 = Dropout(dropout)(graph_conv1)\n",
        "graph_conv2 = GraphConv(n_class, activation='softmax', use_bias=False, kernel_regularizer=l2(regularizer))([dropout_2, A_input])\n",
        "\n",
        "\n",
        "model = Model(inputs=[X_input, A_input], outputs=graph_conv2)\n",
        "optimizer = Adam(learning_rate=lr)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', weighted_metrics=['acc'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJU00RSHlN91",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fitting model\n",
        "\n",
        "validation_data = ([X_tok, A_filter], Y_g, valid_mask)\n",
        "model.fit([X_tok, A_filter],\n",
        "          Y_g,\n",
        "          sample_weight=train_mask,\n",
        "          epochs = 1000,\n",
        "          batch_size=N,\n",
        "          validation_data=validation_data,\n",
        "          shuffle=False,\n",
        "          callbacks=[\n",
        "              EarlyStopping(patience=patience,  restore_best_weights=True)]\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDhGkHA0lQ9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predictions\n",
        "graph_pred = model.predict_on_batch([X_tok, A_filter])\n",
        "\n",
        "# Building final prediction table\n",
        "final_pred2 = np.argmax(graph_pred[np.array(test_mask)], axis=1)\n",
        "print(final_pred2.shape)\n",
        "\n",
        "pred_out2 = te\n",
        "\n",
        "pred_out2['label'] = final_pred2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkX7j9oRlT24",
        "colab_type": "text"
      },
      "source": [
        "## Chebnets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewUNem6NlrE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocess\n",
        "cheb_fltr = ChebConv.preprocess(adj).astype('f4')\n",
        "\n",
        "# Definitions\n",
        "\n",
        "N = adj.shape[0]\n",
        "F = X_tok.shape[-1]\n",
        "n_class = Y_g.shape[-1]\n",
        "dropout = 0.4\n",
        "lr = 0.001\n",
        "epochs = 1000\n",
        "patience = 20\n",
        "regularizer = 0.00005\n",
        "\n",
        "k = 2\n",
        "\n",
        "X_input = Input(shape=(F, ))\n",
        "A_input = Input((N, ), sparse=True)\n",
        "\n",
        "\n",
        "cheb_drop_1 = Dropout(dropout)(X_input)\n",
        "cheb_conv1 = ChebConv(64, activation='relu', use_bias=False, K=k)([cheb_drop_1, A_input])\n",
        "cheb_drop_2 = Dropout(dropout)(cheb_conv1)\n",
        "cheb_conv2 = ChebConv(64, activation='relu', use_bias=False, K=k)([cheb_drop_2, A_input])\n",
        "cheb_drop_3 = Dropout(dropout)(cheb_conv2)\n",
        "cheb_conv3 = ChebConv(n_class, activation='softmax', use_bias=False, K=k)([cheb_drop_3, A_input])\n",
        "\n",
        "model2 = Model(inputs=[X_input, A_input], outputs=cheb_conv3)\n",
        "optimizer = Adam(learning_rate=lr)\n",
        "model2.compile(optimizer=optimizer, loss='categorical_crossentropy', weighted_metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwzoUHw_l0_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fitting model\n",
        "\n",
        "validation_data = ([X_tok, cheb_fltr], Y_g, valid_mask)\n",
        "\n",
        "model2.fit([X_tok, cheb_fltr],\n",
        "          Y_g,\n",
        "          sample_weight=train_mask,\n",
        "          epochs = epochs,\n",
        "          batch_size=N,\n",
        "          validation_data=validation_data,\n",
        "          shuffle=False,\n",
        "          callbacks=[\n",
        "              EarlyStopping(patience=patience,  restore_best_weights=True)]\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vw7FcR5ll5Cu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict\n",
        "\n",
        "# Computing predictions\n",
        "cheb_pred = model2.predict_on_batch([X_g, cheb_fltr])\n",
        "\n",
        "# Building final prediction table\n",
        "final_pred = np.argmax(cheb_pred[np.array(test_mask)], axis=1)\n",
        "print(final_pred.shape)\n",
        "\n",
        "pred_out = te\n",
        "\n",
        "pred_out['label'] = final_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN3HuZrPmABe",
        "colab_type": "text"
      },
      "source": [
        "## GAT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSQeXpBKl_dV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model definition\n",
        "\n",
        "A = add_eye(adj).toarray()\n",
        "\n",
        "X_in = Input(shape=(F, ))\n",
        "A_in = Input((N, ), sparse=False)\n",
        "\n",
        "dropout_1 = Dropout(0.5)(X_in)\n",
        "graph_attention_1 = GraphAttention(8,\n",
        "                                   attn_heads=8,\n",
        "                                   concat_heads=True,\n",
        "                                   dropout_rate=0.5,\n",
        "                                   activation='elu',\n",
        "                                   kernel_regularizer=l2(regularizer),\n",
        "                                   attn_kernel_regularizer=l2(regularizer)\n",
        "                                   )([dropout_1, A_in])\n",
        "dropout_2 = Dropout(0.5)(graph_attention_1)\n",
        "graph_attention_2 = GraphAttention(n_class,\n",
        "                                   attn_heads=1,\n",
        "                                   concat_heads=False,\n",
        "                                   dropout_rate=0.5,\n",
        "                                   activation='softmax',\n",
        "                                   kernel_regularizer=l2(regularizer),\n",
        "                                   attn_kernel_regularizer=l2(regularizer)\n",
        "                                   )([dropout_2, A_in])\n",
        "\n",
        "GAT_model = Model(inputs=[X_in, A_in], outputs=graph_attention_2)\n",
        "optimizer = Adam(lr=0.001)\n",
        "GAT_model.compile(optimizer=optimizer, loss='categorical_crossentropy', weighted_metrics=['acc'])\n",
        "GAT_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffhI_whHmDYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fitting\n",
        "\n",
        "validation_data = ([X_g, A], Y_g, valid_mask)\n",
        "GAT_model.fit([X_g, A], Y_g, sample_weight=train_mask, epochs=1000, batch_size=N, validation_data=validation_data, shuffle=False, callbacks=[EarlyStopping(patience=20, restore_best_weights=True)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFS3fQvam16U",
        "colab_type": "text"
      },
      "source": [
        "## Graph Conv with ARMA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylXs0khXm55s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model definition\n",
        "\n",
        "ARMA_channels = 16\n",
        "share_weights = True\n",
        "ARMA_lr = 0.01\n",
        "ARMA_epochs = 1000\n",
        "ARMA_patience = 100\n",
        "\n",
        "ARMA_fltr = ARMAConv.preprocess(adj).astype('f4')\n",
        "\n",
        "ARMA1 = ARMAConv(ARMA_channels, iterations=1, order=2, share_weights=True, dropout_rate=0.75, activation='elu', gcn_activation='elu',\n",
        "                 kernel_regularizer=l2(regularizer))([X_input, A_input])\n",
        "ARMA_dropout = Dropout(0.5)(ARMA1)\n",
        "ARMA2 = ARMAConv(n_class, iterations=1, order=1, share_weights=True, dropout_rate=0.75, activation='softmax', gcn_activation=None,\n",
        "                       kernel_regularizer=l2(regularizer))([ARMA_dropout, A_input])\n",
        "\n",
        "model3 = Model(inputs=[X_input, A_input], outputs=ARMA2)\n",
        "optimizer = Adam(learning_rate=lr)\n",
        "model3.compile(optimizer=optimizer, loss='categorical_crossentropy', weighted_metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTA6Jwu8m7ax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fitting\n",
        "\n",
        "model3.fit([X_g, A_filter],\n",
        "          Y_g,\n",
        "          sample_weight=train_mask,\n",
        "          epochs = ARMA_epochs,\n",
        "          batch_size=N,\n",
        "          validation_data=validation_data,\n",
        "          shuffle=False,\n",
        "          callbacks=[\n",
        "              EarlyStopping(patience=ARMA_patience,  restore_best_weights=True)]\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}

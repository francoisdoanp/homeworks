{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IFT6758DataScience - Four Geese.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AXp37s67ebM1",
        "f-TPAVPxCr5N",
        "WhTj18YxCwOi",
        "9TCGLXKtEDGk",
        "LfBY15zeEhKb",
        "QUq5F502cfPl",
        "xq4cXbAvfBof",
        "4ITeSvhK6zUh",
        "1Ka5KCoYgMYr",
        "cDCXxXRc7rN8"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nkXzxAj1JGe"
      },
      "source": [
        "# IFT6758 Data Science Kaggle competition\n",
        "\n",
        "\n",
        "Team - Four Geese\n",
        ">\n",
        "Name | Student ID | Kaggle Username\n",
        "--- | --- | ---\n",
        "François Doan-Pope | 20054932 | franckdp\n",
        "Sina Sarparast | 20181256 | Sina1374\n",
        "Joshua Amelia | 20146389 | joshuaamelia\n",
        "Carryll Yu Ping Liang | 20051432 | yu4kaggle\n",
        ">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBH_xqCF19PJ"
      },
      "source": [
        "# Grabbing data from Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzHWNeEb2DpK"
      },
      "source": [
        "You can skip this section and download the dataset manually.\n",
        "\n",
        "This section is for google colab only. First install kaggle api. Then from the profile setting activate the data api and download your Kaggle key (.json file)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ6ikoHY7XaO"
      },
      "source": [
        "update it as well"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tthUNj_lDuJf",
        "outputId": "9e984039-c561-438e-fdba-5561933b0599"
      },
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kaggle\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/33/365c0d13f07a2a54744d027fe20b60dacdfdfb33bc04746db6ad0b79340b/kaggle-1.5.10.tar.gz (59kB)\n",
            "\r\u001b[K     |█████▌                          | 10kB 23.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 20kB 17.4MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 30kB 15.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 40kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 51kB 11.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 6.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.10-cp36-none-any.whl size=73269 sha256=de99bcdd3d81454fdc5d0930e12e971923ac7413d9485b42b0b0211a3e0eb3a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/d1/7e/6ce09b72b770149802c653a02783821629146983ee5a360f10\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Found existing installation: kaggle 1.5.10\n",
            "    Uninstalling kaggle-1.5.10:\n",
            "      Successfully uninstalled kaggle-1.5.10\n",
            "Successfully installed kaggle-1.5.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puvO1LSryfae"
      },
      "source": [
        "Upload the key here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCkgewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwogICAgICBwZXJjZW50LnRleHRDb250ZW50ID0KICAgICAgICAgIGAke01hdGgucm91bmQoKHBvc2l0aW9uIC8gZmlsZURhdGEuYnl0ZUxlbmd0aCkgKiAxMDApfSUgZG9uZWA7CiAgICB9CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "LRHtdMsPOpBz",
        "outputId": "1af327d4-8127-43c4-c81e-9a0244dec68b"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ecfcc728-b2ff-413e-a35e-c040d0c8eeb8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ecfcc728-b2ff-413e-a35e-c040d0c8eeb8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle-3.json to kaggle-3.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle-3.json': b'{\"username\":\"sina1374\",\"key\":\"fc9770d07c6c9c1d817ce2340a15a5cc\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9WCBgwLOx8i"
      },
      "source": [
        "! mkdir ~/.kaggle\n",
        "\n",
        "! cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDA-dKcoPJZY"
      },
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJuigTfQ2dyf"
      },
      "source": [
        "download dataset using the key and the following command"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BRiRB3CPkXJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a82fdc8-efea-4f77-a5da-b32ebff249a9"
      },
      "source": [
        "!kaggle competitions download -c ift6758-a20"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ift6758-a20.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmi8GrJmP43q"
      },
      "source": [
        "! unzip ift6758-a20.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXp37s67ebM1"
      },
      "source": [
        "# Access data from google drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DeDiZAXegzZ"
      },
      "source": [
        "This section has nothing to do with the competition. I'm just trying to make things easier for everyone."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msIVKWq66Cdq"
      },
      "source": [
        "You may need to install pydrive first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSi6h0H052U1",
        "outputId": "9559ccf6-f116-460b-9f2a-006a7dad62e8"
      },
      "source": [
        "! pip install pydrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (3.13)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from pydrive) (1.7.12)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.2.8)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.17.4)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (1.15.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.8)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (3.0.1)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (1.17.2)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->pydrive) (50.3.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->pydrive) (4.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CY6Wi94yesUB"
      },
      "source": [
        "We try to access the data set as a shared drive file. Here are the Drive file ids:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZQ7T3dMlvrL"
      },
      "source": [
        "train_csv_id = '13C3PPMOX8ZmAqUYVTugknL72yk6sblA8'\n",
        "test_csv_id = '1rKugQNSo_6iILOsriGbS90fOCV5bfh9l'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IlTZxlDnS15"
      },
      "source": [
        "We authenticate and then create a file reader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm9k6Ffvi_56"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0z0gvM6nfC5"
      },
      "source": [
        "Download the files to the local colab storage.\n",
        "You need to do this each time you open the colab file, since google colab doesn't store files between sessions. Refresh the files screen to see downloaded files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgYRoc7_jIPa"
      },
      "source": [
        "train_csv = drive.CreateFile({'id': train_csv_id})\n",
        "train_csv.GetContentFile('train.csv', mimetype='csv')\n",
        "test_csv = drive.CreateFile({'id': test_csv_id})\n",
        "test_csv.GetContentFile('test.csv', mimetype='csv')\n",
        "# print('Downloaded content \"{}\"'.format(downloaded.GetContentString()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8PPgMmM2m58"
      },
      "source": [
        "# Import libraries and data & seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1mhthhazGyL"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from numpy.random import seed\n",
        "import tensorflow as tf\n",
        "\n",
        "seed(12)\n",
        "tf.random.set_seed(24)\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt  # Matlab-style plotting\n",
        "import seaborn as sns\n",
        "color = sns.color_palette()\n",
        "sns.set_style('darkgrid')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-TPAVPxCr5N"
      },
      "source": [
        "# Model retained for Kaggle and its preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlCgu0PDIeND"
      },
      "source": [
        "read the CSV files to pandas dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "envpr2R1IeND"
      },
      "source": [
        "train_set = pd.read_csv('train.csv')\n",
        "test_set = pd.read_csv('test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsTrMU_HCpwK"
      },
      "source": [
        "## Preprocessing on user's locations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhTj18YxCwOi"
      },
      "source": [
        "### Functions to extract locations (read note)\r\n",
        "\r\n",
        "Note: Without API key, this will not work. Skip to \"Loading back our extracted data\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eguI2n4DCyb2"
      },
      "source": [
        "from geopy.geocoders import GoogleV3\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('drive')\r\n",
        "\r\n",
        "path = 'define your drive path'\r\n",
        "\r\n",
        "train_location = train_set[['Id', 'Location', 'User Time Zone']]\r\n",
        "test_location = test_set[['Id', 'Location', 'User Time Zone']]\r\n",
        "\r\n",
        "geolocator = GoogleV3(api_key='ENTER YOUR API KEY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlfWYR97C2f6"
      },
      "source": [
        "def extract_country(address):\r\n",
        "  split = address.rsplit(',', 1)\r\n",
        "  \r\n",
        "  if len(split) != 1:\r\n",
        "    country = split[1].strip()\r\n",
        "  else:\r\n",
        "    country = split[0]\r\n",
        "\r\n",
        "  return country\r\n",
        "\r\n",
        "def find_country(array):\r\n",
        "\r\n",
        "  country_list = []\r\n",
        "\r\n",
        "  for value in array:\r\n",
        "    if pd.notna(value):\r\n",
        "      loc = geolocator.geocode(value, language='en')\r\n",
        "      if loc:\r\n",
        "        country = extract_country(loc.address)\r\n",
        "        country_list.append(country)\r\n",
        "      else:\r\n",
        "        country_list.append(None)\r\n",
        "    else:\r\n",
        "      country_list.append(None)\r\n",
        "\r\n",
        "  return country_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwubAvdJDKFj"
      },
      "source": [
        "Extracting location from location column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh57Zyl8DFeF"
      },
      "source": [
        "# Training set\r\n",
        "\r\n",
        "train_location_ext = find_country(train_location.iloc[:,1].values)\r\n",
        "df_temp = pd.DataFrame(train_location_ext)\r\n",
        "df_location = pd.concat([train_set.iloc[:,0], df_temp], axis=1)\r\n",
        "df_location.to_csv(path+'/train_location.csv')\r\n",
        "\r\n",
        "# Testing set\r\n",
        "\r\n",
        "df_temp_test = pd.DataFrame(test_location_ext)\r\n",
        "df_location_test = pd.concat([test_set.iloc[:,0], df_temp_test], axis=1)\r\n",
        "df_location_test.to_csv(path+'/test_location.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo17OQBbDqaI"
      },
      "source": [
        "Extraction location from time zone column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI0yua8zDsis"
      },
      "source": [
        "# Training set\r\n",
        "\r\n",
        "df_location2 = pd.merge(df_location, train_location, left_on='Id', right_on='Id', how='left')\r\n",
        "tmz_location = df_location2[df_location2[0].isnull()]\r\n",
        "train_tmz_location = find_country(tmz_location.iloc[:,3].values)\r\n",
        "df_temp_tmz = pd.DataFrame(train_tmz_location, columns=['TMZ Location'])\r\n",
        "df_location_tmz = pd.concat([tmz_location.reset_index(), df_temp_tmz.reset_index()], axis=1)\r\n",
        "\r\n",
        "df_location_tmz.to_csv(path+'/train_location_tmz.csv')\r\n",
        "\r\n",
        "# Testing set\r\n",
        "\r\n",
        "df_location_test2 = pd.merge(df_location_test, test_location, left_on='Id', right_on='Id', how='left')\r\n",
        "tmz_location_test = df_location_test2[df_location_test2[0].isnull()]\r\n",
        "test_tmz_location = find_country(tmz_location_test.iloc[:,3].values)\r\n",
        "df_temp_tmz_test = pd.DataFrame(test_tmz_location, columns=['TMZ Location'])\r\n",
        "df_location_tmz_test = pd.concat([tmz_location_test.reset_index(), df_temp_tmz_test.reset_index()], axis=1)\r\n",
        "\r\n",
        "df_location_tmz_test.to_csv(path+'/test_location_tmz.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TCGLXKtEDGk"
      },
      "source": [
        "### Loading back our extracted data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOuMKXtrEFRZ"
      },
      "source": [
        "tmz_fields = ['Id', 'TMZ Location']\r\n",
        "loc_names = ['Id', 'Location']\r\n",
        "\r\n",
        "train_loc = pd.read_csv('https://raw.githubusercontent.com/francoisdoanp/projects/master/DS/train_location.csv', usecols=['Id', '0']).set_index('Id')\r\n",
        "train_tmz = pd.read_csv('https://raw.githubusercontent.com/francoisdoanp/projects/master/DS/train_location_tmz.csv', usecols=tmz_fields).set_index('Id')\r\n",
        "test_loc = pd.read_csv('https://raw.githubusercontent.com/francoisdoanp/projects/master/DS/test_location.csv').drop('Unnamed: 0', 1).set_index('Id')\r\n",
        "test_tmz = pd.read_csv('https://raw.githubusercontent.com/francoisdoanp/projects/master/DS/test_location_tmz.csv', usecols=tmz_fields).set_index('Id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xr7rjtpoEIpT"
      },
      "source": [
        "### Transformations on location data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdKD5SCdELrv"
      },
      "source": [
        "tmpt = train_loc['0']\r\n",
        "tmpt.update(train_tmz['TMZ Location'])\r\n",
        "train_loc['0'] = tmpt\r\n",
        "train_loc.reset_index(inplace=True)\r\n",
        "\r\n",
        "tmpte = test_loc['0']\r\n",
        "tmpte.update(test_tmz['TMZ Location'])\r\n",
        "test_loc['0'] = tmpte\r\n",
        "test_loc.reset_index(inplace=True)\r\n",
        "\r\n",
        "# Manually replacing erroneous values\r\n",
        "\r\n",
        "train_setna = ['International Date Line', '02000', '127051', '3 Chome−中２−１', '1591-9 경기도 안양시 동안구','125009', '603000','서울특별시 강남구 역삼동 719-5', '109012']\r\n",
        "test_setna = ['번지 102호 1041-5 조은빌딩', '117105', '6-chōme−27−番３０号 新宿イーストサイドスクエア', '1−７９ アリオ川口1階', '690002', '02000']\r\n",
        "loc_replace = {'Dubai - United Arab Emirates': 'United Arab Emirates', 'UK':'United Kingdom', 'Jeddah Saudi Arabia': 'Saudi Arabia', 'United States': 'USA', 'Al Qassim Saudi Arabia': 'Saudi Arabia', \r\n",
        "                 'Sheikh Mohammed bin Rashid Blvd - Dubai - United Arab Emirates': 'United Arab Emirates', \r\n",
        "                 'Sheikh Zayed Rd Near Financial Metro Station - Trade CentreTrade Centre 1 - Dubai - United Arab Emirates': 'United Arab Emirates', 'Singapore 207542': 'Singapore', \r\n",
        "                 'Sharjah - United Arab Emirates': 'United Arab Emirates', 'Al Sufouh 1 - Dubai - United Arab Emirates': 'United Arab Emirates', \r\n",
        "                 'Yas Island - Abu Dhabi - United Arab Emirates': 'United Arab Emirates', 'Al SufouhDubai Media City - Dubai - United Arab Emirates': 'United Arab Emirates', 'Singapore 039593': 'Singapore',\r\n",
        "                 'Abu Dhabi - United Arab Emirates': 'United Arab Emirates', '98000 Monaco': 'Monaco', 'Persian Gulf (also known as the Arabian Gulf)': 'Persian Gulf', '4-chōme−4−番２号 東山ビルディング 5階': np.NaN,\r\n",
        "               'The Bahamas': 'Bahamas', 'U.S. Virgin Islands': 'United States Virgin Islands'}\r\n",
        "\r\n",
        "train_loc['0'].replace(train_setna, np.NaN, inplace=True)\r\n",
        "test_loc['0'].replace(test_setna, np.NaN, inplace=True)\r\n",
        "train_loc.replace({'0': loc_replace}, inplace=True)\r\n",
        "test_loc.replace({'0': loc_replace}, inplace=True)\r\n",
        "\r\n",
        "# Replacing nan's with unknown\r\n",
        "\r\n",
        "train_loc.fillna('Unknown', inplace=True)\r\n",
        "test_loc.fillna('Unknown', inplace=True)\r\n",
        "\r\n",
        "train_loc.columns = ['Id', 'Location']\r\n",
        "test_loc.columns = ['Id', 'Location']\r\n",
        "\r\n",
        "# Replacing countries that are not in the test set as unknown\r\n",
        "known_locations = train_loc['Location'].unique()\r\n",
        "test_loc[~test_loc.Location.isin(known_locations)] = 'Unknown'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPz3jRpuEp68"
      },
      "source": [
        "train_set['Location'] = train_loc['Location']\n",
        "test_set['Location'] = test_loc['Location']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfBY15zeEhKb"
      },
      "source": [
        "## Preprocessing on color features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCwrL-hpElG2"
      },
      "source": [
        "from PIL import ImageColor\r\n",
        "\r\n",
        "# Creating a dataframe for each features containing color values\r\n",
        "train_texcolor = train_set['Profile Text Color']\r\n",
        "train_pagcolor = train_set['Profile Page Color']\r\n",
        "train_thmcolor = train_set['Profile Theme Color']\r\n",
        "\r\n",
        "test_texcolor = test_set['Profile Text Color']\r\n",
        "test_pagcolor = test_set['Profile Page Color']\r\n",
        "test_thmcolor = test_set['Profile Theme Color']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5gcUXLAEo6Q"
      },
      "source": [
        "# Functions to extract colors from HEX values\r\n",
        "def get_color(color):\r\n",
        "  rgb = ImageColor.getcolor('#'+ str(color), \"RGB\")\r\n",
        "\r\n",
        "  return [x/255.0 for x in rgb]\r\n",
        "\r\n",
        "def get_all_colors(data):\r\n",
        "\r\n",
        "  color_array = []\r\n",
        "\r\n",
        "  for color in data:\r\n",
        "    temp = get_color(color)\r\n",
        "    color_array.append(temp)\r\n",
        "\r\n",
        "  return np.array(color_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnTjnUmsEtQ-"
      },
      "source": [
        "# Clustering colors\r\n",
        "from sklearn.cluster import KMeans\r\n",
        "\r\n",
        "def cluster_colors(array, k):\r\n",
        "  kmeans = KMeans(k)\r\n",
        "  kmeans.fit(array)\r\n",
        "  clustered_array = kmeans.predict(array)\r\n",
        "\r\n",
        "  return kmeans, clustered_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lKmEAh-EqI0"
      },
      "source": [
        "from sklearn_pandas import CategoricalImputer\r\n",
        "\r\n",
        "# Imputing values (which is the same as SimpleImputer based on version 2.0.0 update of sk_learnpandas)\r\n",
        "imputer_cat = CategoricalImputer()\r\n",
        "train_texcolor = imputer_cat.fit_transform(train_texcolor)\r\n",
        "train_pagcolor = imputer_cat.fit_transform(train_pagcolor)\r\n",
        "train_thmcolor = imputer_cat.fit_transform(train_thmcolor)\r\n",
        "\r\n",
        "train_texcolor = get_all_colors(train_texcolor)\r\n",
        "train_pagcolor = get_all_colors(train_pagcolor)\r\n",
        "train_thmcolor = get_all_colors(train_thmcolor)\r\n",
        "\r\n",
        "test_texcolor = imputer_cat.fit_transform(test_texcolor)\r\n",
        "test_pagcolor = imputer_cat.fit_transform(test_pagcolor)\r\n",
        "test_thmcolor = imputer_cat.fit_transform(test_thmcolor)\r\n",
        "\r\n",
        "test_texcolor = get_all_colors(test_texcolor)\r\n",
        "test_pagcolor = get_all_colors(test_pagcolor)\r\n",
        "test_thmcolor = get_all_colors(test_thmcolor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPjArtrhEsFP"
      },
      "source": [
        "kmeans_tex, train_tcolor = cluster_colors(train_texcolor, 12)\r\n",
        "kmeans_pag, train_pcolor = cluster_colors(train_pagcolor, 12)\r\n",
        "kmeans_thm, train_thcolor = cluster_colors(train_thmcolor, 12)\r\n",
        "\r\n",
        "test_tcolor = pd.DataFrame(kmeans_tex.predict(test_texcolor), columns=['text_color'])\r\n",
        "test_pcolor =  pd.DataFrame(kmeans_pag.predict(test_pagcolor), columns=['page_color'])\r\n",
        "test_thcolor =  pd.DataFrame(kmeans_thm.predict(test_thmcolor), columns=['theme_color'])\r\n",
        "\r\n",
        "train_tcolor = pd.DataFrame(train_tcolor, columns=['text_color'])\r\n",
        "train_pcolor = pd.DataFrame(train_pcolor, columns=['page_color'])\r\n",
        "train_thcolor = pd.DataFrame(train_thcolor, columns=['theme_color'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_9-xhM4EyIu"
      },
      "source": [
        "train_set.drop(columns=['Profile Text Color', 'Profile Page Color', 'Profile Theme Color'], inplace=True)\r\n",
        "test_set.drop(columns=['Profile Text Color', 'Profile Page Color', 'Profile Theme Color'], inplace=True)\r\n",
        "\r\n",
        "train_set = pd.concat([train_set, train_tcolor, train_pcolor, train_thcolor], axis=1)\r\n",
        "test_set = pd.concat([test_set, test_tcolor, test_pcolor, test_thcolor], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUq5F502cfPl"
      },
      "source": [
        "## Preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfVUGVgm4wG2"
      },
      "source": [
        "import time\n",
        "from sklearn.experimental import enable_iterative_imputer \n",
        "from sklearn.impute import SimpleImputer, IterativeImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Function to impute missing values using IterativeImputer (more precisely, Bayesian Ridge)\n",
        "def imputeMissing(df):\n",
        "    cols =[\n",
        "        'Profile Cover Image Status',\n",
        "    ]\n",
        "    df[cols]=df[cols].fillna('missing')\n",
        "\n",
        "    cols =[\n",
        "       'Num of Followers',\n",
        "        'Num of People Following',\n",
        "        'Num of Status Updates',\n",
        "        'Num of Direct Messages',\n",
        "        'Avg Daily Profile Visit Duration in seconds',\n",
        "        'Avg Daily Profile Clicks',\n",
        "    ]\n",
        "    imp = IterativeImputer(missing_values=np.nan, initial_strategy='median', imputation_order='random', add_indicator=True)\n",
        "    imp.fit(df[cols])\n",
        "\n",
        "    df_tmp = imp.transform(df[cols])\n",
        "    return df_tmp\n",
        "\n",
        "# Function to modify some peculiar values\n",
        "def cleanData(df):\n",
        "    df['Location Public Visibility'] = df['Location Public Visibility'].str.lower()\n",
        "    indicies = df['Location Public Visibility']=='??'\n",
        "    df.loc[indicies,'Location Public Visibility'] = df['Location Public Visibility'].mode().item()\n",
        "    indicies = df['Profile Category']==' ' \n",
        "    df.loc[indicies,'Profile Category'] = 'unknown'\n",
        "    return df\n",
        "\n",
        "# Functions for some feature engineering\n",
        "# Length of username, boolean indicator for presence of personal URL and year of profile creation\n",
        "def transformData(df):\n",
        "    df['len User Name'] = df['User Name'].str.len()\n",
        "    index = df['Personal URL'].isnull()\n",
        "    df['Has Personal URL'] = 0\n",
        "    df.loc[index,'Has Personal URL'] = 1\n",
        "    df['Year Of Profile Creation'] = pd.DatetimeIndex(df['Profile Creation Timestamp']).year\n",
        "\n",
        "    return df\n",
        "\n",
        "# Transformations on some numerical features to have a more Gaussian-like distribution.\n",
        "def getPowerTransformer(train_set):\n",
        "    to_scale = ['Num of Followers',\n",
        "                'Num of People Following',\n",
        "                'Num of Status Updates',\n",
        "                'Num of Direct Messages',\n",
        "                'Total Daily Profile Visit Duration in seconds',\n",
        "                'Total Clicks',\n",
        "                'Avg Followers Per Year'\n",
        "        ]\n",
        "\n",
        "    pt = PowerTransformer()\n",
        "    pt.fit(train_set[to_scale]) \n",
        "    return pt\n",
        "\n",
        "def getMinMaxScaler(train_set):\n",
        "    to_scale = [\n",
        "                'Avg Daily Profile Visit Duration in seconds',\n",
        "                'Avg Daily Profile Clicks',\n",
        "                'len User Name'\n",
        "            ]\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(train_set[to_scale])\n",
        "    return scaler\n",
        "\n",
        "# One-hot encoding for three labels\n",
        "def getLabelEncoders(train_set):\n",
        "    cols = [\n",
        "            'Profile Cover Image Status',\n",
        "            'Is Profile View Size Customized?',\n",
        "            'Location Public Visibility']\n",
        "    labelEncoders = {}\n",
        "    for col in cols:\n",
        "        labelEncoders[col] = LabelEncoder()\n",
        "        labelEncoders[col].fit( train_set[col] )\n",
        "    return labelEncoders\n",
        "\n",
        "# Putting it all together\n",
        "def encodeAndScale(df, labelEncoders, powerTransformer, minMaxScaler):\n",
        "    cols = [\n",
        "            'User Language',\n",
        "        'Profile Category',\n",
        "        'Profile Verification Status',\n",
        "        'Year Of Profile Creation',\n",
        "        'theme_color',\n",
        "        'text_color',\n",
        "        'page_color',\n",
        "        'Location'\n",
        "    ]\n",
        "    df = pd.get_dummies(df, columns=cols)\n",
        "\n",
        "    cols = [\n",
        "            'Profile Cover Image Status',\n",
        "            'Is Profile View Size Customized?',\n",
        "            'Location Public Visibility']\n",
        "    for col in cols:\n",
        "        df[col] = labelEncoders[col].transform( df[col] )\n",
        "\n",
        "    to_remove = ['Id',\n",
        "                'User Name',\n",
        "                'Personal URL',\n",
        "                'Profile Image',\n",
        "                'UTC Offset',\n",
        "                'Profile Creation Timestamp',\n",
        "                ] \n",
        "\n",
        "    df = df.drop(columns=to_remove)\n",
        "\n",
        "    to_scale = ['Num of Followers',\n",
        "                'Num of People Following',\n",
        "                'Num of Status Updates',\n",
        "                'Num of Direct Messages',\n",
        "                'Total Daily Profile Visit Duration in seconds',\n",
        "                'Total Clicks',\n",
        "                'Avg Followers Per Year'\n",
        "        ]\n",
        "    df[to_scale] = powerTransformer.transform(df[to_scale])\n",
        "\n",
        "\n",
        "    to_scale = [\n",
        "                    'Avg Daily Profile Visit Duration in seconds',\n",
        "                    'Avg Daily Profile Clicks',\n",
        "                    'len User Name'\n",
        "                ]\n",
        "    df[to_scale] = minMaxScaler.transform(df[to_scale])\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq4cXbAvfBof"
      },
      "source": [
        "## Applying the preprocessing on our training and testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7YRmMi7fFqc"
      },
      "source": [
        "# Dropping location and user time zone and adding the country extracted for all observations\r\n",
        "\r\n",
        "train_set.drop(columns=['Location', 'User Time Zone'], axis=1, inplace=True)\r\n",
        "test_set.drop(columns=['Location', 'User Time Zone'], axis=1, inplace=True)\r\n",
        "\r\n",
        "train_set = pd.concat([train_set, train_loc], axis=1)\r\n",
        "test_set = pd.concat([test_set, test_loc], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DD66hzZhfPvr"
      },
      "source": [
        "# Applying our preprocessing functions\r\n",
        "\r\n",
        "trainDf = train_set.copy()\r\n",
        "testDf = test_set.copy()\r\n",
        "\r\n",
        "train_tmp = imputeMissing(trainDf)\r\n",
        "trainDf['Num of Followers'] = train_tmp[:,0]\r\n",
        "trainDf['Num of People Following'] = train_tmp[:,1]\r\n",
        "trainDf['Num of Status Updates'] = train_tmp[:,2]\r\n",
        "trainDf['Num of Direct Messages'] = train_tmp[:,3]\r\n",
        "trainDf['Avg Daily Profile Visit Duration in seconds'] = train_tmp[:,4]\r\n",
        "trainDf['Avg Daily Profile Clicks'] = train_tmp[:,5]\r\n",
        "trainDf['indc1'] = train_tmp[:,6]\r\n",
        "trainDf['indc2'] = train_tmp[:,7]\r\n",
        "\r\n",
        "# Feature engineering: Adding three variables from calculations based on available features.\r\n",
        "trainDf['Total Daily Profile Visit Duration in seconds'] = trainDf['Avg Daily Profile Visit Duration in seconds'] * trainDf['Avg Daily Profile Clicks']\r\n",
        "trainDf['Total Clicks'] = trainDf['Avg Daily Profile Clicks'] * 365 * (2020-pd.DatetimeIndex(trainDf['Profile Creation Timestamp']).year)\r\n",
        "trainDf['Avg Followers Per Year'] = trainDf['Num of Followers']/(2020-pd.DatetimeIndex(trainDf['Profile Creation Timestamp']).year)\r\n",
        "\r\n",
        "trainDf = cleanData(trainDf)\r\n",
        "trainDf = transformData(trainDf)\r\n",
        "\r\n",
        "les = getLabelEncoders(trainDf)\r\n",
        "pt = getPowerTransformer(trainDf)\r\n",
        "mms = getMinMaxScaler(trainDf)\r\n",
        "trainDf = encodeAndScale(trainDf, les, pt, mms)\r\n",
        "x_train = trainDf.drop(columns=[\"Num of Profile Likes\"])\r\n",
        "y_train = trainDf[\"Num of Profile Likes\"]\r\n",
        "\r\n",
        "test_tmp = imputeMissing(testDf)\r\n",
        "\r\n",
        "testDf['Num of Followers'] = test_tmp[:,0]\r\n",
        "testDf['Num of People Following'] = test_tmp[:,1]\r\n",
        "testDf['Num of Status Updates'] = test_tmp[:,2]\r\n",
        "testDf['Num of Direct Messages'] = test_tmp[:,3]\r\n",
        "testDf['Avg Daily Profile Visit Duration in seconds'] = test_tmp[:,4]\r\n",
        "testDf['Avg Daily Profile Clicks'] = test_tmp[:,5]\r\n",
        "testDf['indc1'] = test_tmp[:,6]\r\n",
        "testDf['indc2'] = test_tmp[:,7]\r\n",
        "\r\n",
        "# Feature engineering: Adding three variables from calculations based on available features.\r\n",
        "testDf['Total Daily Profile Visit Duration in seconds'] = testDf['Avg Daily Profile Visit Duration in seconds'] * testDf['Avg Daily Profile Clicks']\r\n",
        "testDf['Total Clicks'] = testDf['Avg Daily Profile Clicks'] * 365 * (2020-pd.DatetimeIndex(testDf['Profile Creation Timestamp']).year)\r\n",
        "testDf['Avg Followers Per Year'] = testDf['Num of Followers']/(2020-pd.DatetimeIndex(testDf['Profile Creation Timestamp']).year)\r\n",
        "\r\n",
        "testDf = cleanData(testDf)\r\n",
        "testDf = transformData(testDf)\r\n",
        "x_test = encodeAndScale(testDf, les, pt, mms)\r\n",
        "missingCols = set(x_train.columns) - set(x_test.columns)\r\n",
        "for c in missingCols:\r\n",
        "    x_test[c] = 0\r\n",
        "x_test = x_test[x_train.columns]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ITeSvhK6zUh"
      },
      "source": [
        "## Loading the images from the competition\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olsV52yQ7Sq5"
      },
      "source": [
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "# Function to load images from the file name in our dataframe\n",
        "# Input: - Dataframe containing the filenames in column Profile Image\n",
        "#        - The path where images are located.\n",
        "# Output: Numpy array \n",
        "# Note: the images loaded from the Dataframe needs to be in the same order as the features from the training/testing set.\n",
        "\n",
        "def load_images(dataframe, imagesPath):\n",
        "\n",
        "  inputImages = []\n",
        "\n",
        "  for fileName in dataframe.values:\n",
        "    image = load_img(imagesPath + fileName[0])\n",
        "    inputImages.append(img_to_array(image))\n",
        "\n",
        "  return np.array(inputImages)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LsDAxeNFGn5"
      },
      "source": [
        "# Loading dataframe with images filename\n",
        "\n",
        "train_images = pd.read_csv('train.csv', usecols=['Profile Image'])\n",
        "test_images =  pd.read_csv('test.csv', usecols=['Profile Image'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pvxNvqG7WqV",
        "outputId": "b079a159-6882-4b45-f858-85e21bbc71c7"
      },
      "source": [
        "# Define the image's paths\n",
        "path_train = './train_profile_images/profile_images_train/'\n",
        "path_test = './test_profile_images/profile_images_test/'\n",
        "\n",
        "# Import images based on DataFrame's Profile Image feature\n",
        "training_imgs = load_images(train_images, path_train)\n",
        "print('Training images shape: {}'.format(training_imgs.shape))\n",
        "testing_imgs = load_images(test_images, path_test)\n",
        "print('Training images shape: {}'.format(testing_imgs.shape))\n",
        "\n",
        "# Normalizing pixels\n",
        "training_imgs, testing_imgs = (training_imgs-127.5)/127.5, (testing_imgs-127.5)/127.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training images shape: (7500, 32, 32, 3)\n",
            "Training images shape: (2500, 32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzDG4Svn3nfb",
        "outputId": "ba645fe0-5b22-4420-a233-7f907cdfee0d"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "trainX, valX, trainImg, valImg, trainY, valY = train_test_split(x_train, training_imgs,\n",
        "                                                                y_train, test_size=0.20, random_state=42)\n",
        "\n",
        "print(\"x_train:\", x_train.shape)\n",
        "print(\"training_imgs:\", training_imgs.shape)\n",
        "print(\"y_train:\", y_train.shape)\n",
        "print(\"=\"*50)\n",
        "print(\"trainX:\", trainX.shape)\n",
        "print(\"valX:\", valX.shape)\n",
        "print(\"trainImg:\", trainImg.shape)\n",
        "print(\"valImg:\", valImg.shape)\n",
        "print(\"trainY:\", trainY.shape)\n",
        "print(\"valY:\", valY.shape)\n",
        "print(\"=\"*50)\n",
        "print(\"x_test:\", x_test.shape)\n",
        "print(\"testing_imgs\",testing_imgs.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train: (7500, 236)\n",
            "training_imgs: (7500, 32, 32, 3)\n",
            "y_train: (7500,)\n",
            "==================================================\n",
            "trainX: (6000, 236)\n",
            "valX: (1500, 236)\n",
            "trainImg: (6000, 32, 32, 3)\n",
            "valImg: (1500, 32, 32, 3)\n",
            "trainY: (6000,)\n",
            "valY: (1500,)\n",
            "==================================================\n",
            "x_test: (2500, 236)\n",
            "testing_imgs (2500, 32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2REUIb697oRG"
      },
      "source": [
        "## Neural Network model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ka5KCoYgMYr"
      },
      "source": [
        "### Image Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYunyp1JgO7o"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\r\n",
        "\r\n",
        "# Applying some image transformations\r\n",
        "datagen = ImageDataGenerator(\r\n",
        "          featurewise_center=False,  \r\n",
        "          samplewise_center=True,  \r\n",
        "          featurewise_std_normalization=False,   \r\n",
        "          samplewise_std_normalization=False,  \r\n",
        "          zca_whitening=False,  \r\n",
        "          rotation_range=5,\r\n",
        "          width_shift_range=0.1,\r\n",
        "          height_shift_range=0.1, \r\n",
        "          horizontal_flip=True,  \r\n",
        "          vertical_flip=False,\r\n",
        "          brightness_range=[0.1,1.0],\r\n",
        "          zoom_range=[0.2,1.0],\r\n",
        "          rescale=1.0/255.0)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDCXxXRc7rN8"
      },
      "source": [
        "### CNN + MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSry_57vgVaT"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "trainX, valX, trainImg, valImg, trainY, valY = train_test_split(x_train, training_imgs, y_train, test_size=0.3, random_state=42)\r\n",
        "\r\n",
        "generator_train = datagen.flow([trainImg, trainX], y=trainY, shuffle=True)\r\n",
        "generator_val = datagen.flow([valImg, valX], valY, shuffle=True)\r\n",
        "generator_test = datagen.flow([testing_imgs, x_test], shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVYeCYGI7uJP",
        "outputId": "bdfa383f-6a0a-483f-999a-79f208e9347c"
      },
      "source": [
        "from keras.models import Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Concatenate, Input\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define inputs shape\n",
        "num_features = x_train.shape[1]\n",
        "size_imgs = training_imgs.shape[1:]\n",
        "\n",
        "image_input = Input((size_imgs))\n",
        "feature_input = Input((num_features,))\n",
        "\n",
        "# Define CNN architecture - adjust hyperparameters as needed\n",
        "\n",
        "conv_layer = Conv2D(32, (3,3), activation='relu')(image_input)\n",
        "max_pool_layer = MaxPooling2D((2,2))(conv_layer)\n",
        "conv_layer2 = Conv2D(16, (3, 3), activation='relu')(max_pool_layer)\n",
        "max_pool_layer2 = MaxPooling2D((2,2))(conv_layer2)\n",
        "flat_layer = Flatten()(max_pool_layer2)\n",
        "dense_layer_cnn = Dense(8, activation='relu')(flat_layer)\n",
        "\n",
        "# Define MLP Architecture - adjust hyperparameters as needed\n",
        "\n",
        "dense_layer2 = Dense(128, activation='relu')(feature_input)\n",
        "dropout2 = Dropout(0.5)(dense_layer2)\n",
        "dense_layer3 = Dense(64, activation='relu')(feature_input)\n",
        "dropout3 = Dropout(0.5)(dense_layer3)\n",
        "dense_layer4 = Dense(32, activation='relu')(dropout3)\n",
        "\n",
        "# Concatenate features (or MLP) and CNN - adjust hyperparameters as needed\n",
        "\n",
        "concatenate_layer = Concatenate()([dense_layer_cnn, dense_layer4])\n",
        "dense_layer_l3 = Dense(64, activation='relu')(concatenate_layer)\n",
        "dropout_l = Dropout(0.5)(dense_layer_l3)\n",
        "dense_layer_l4 = Dense(32, activation='relu')(dropout_l)\n",
        "dropout_l2 = Dropout(0.5)(dense_layer_l4)\n",
        "dense_layer_l5 = Dense(16, activation='relu')(dropout_l2)\n",
        "dropout_l3 = Dropout(0.5)(dense_layer_l5)\n",
        "dense_layer_l6 = Dense(8, activation='relu')(dropout_l3)\n",
        "dropout_l4 = Dropout(0.5)(dense_layer_l6)\n",
        "output = Dense(1, activation='relu')(dropout_l4)\n",
        "\n",
        "model_cnnmlp = Model(inputs=[image_input, feature_input], outputs=output)\n",
        "\n",
        "model_cnnmlp.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 30, 30, 32)   896         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 15, 15, 32)   0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 13, 13, 16)   4624        max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            [(None, 236)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 6, 6, 16)     0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 64)           15168       input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 576)          0           max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 64)           0           dense_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 8)            4616        flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 32)           2080        dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 40)           0           dense_9[0][0]                    \n",
            "                                                                 dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 64)           2624        concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 64)           0           dense_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 32)           2080        dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 32)           0           dense_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 16)           528         dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 16)           0           dense_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 8)            136         dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 8)            0           dense_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 1)            9           dropout_11[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 32,761\n",
            "Trainable params: 32,761\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaaueEgN8i9d"
      },
      "source": [
        "from keras.utils import plot_model\n",
        "\n",
        "# Plot the model architecture\n",
        "plot_model(model_cnnmlp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPYalfABgg4i"
      },
      "source": [
        "# Defining our RMSLE loss function\r\n",
        "\r\n",
        "def RMSLE(y_pred, y_true): \r\n",
        "  y_pred= tf.cast(y_pred, tf.float64) \r\n",
        "  y_true= tf.cast(y_true, tf.float64) \r\n",
        "  y_pred=tf.nn.relu(y_pred) \r\n",
        "  return tf.sqrt(tf.reduce_mean(tf.math.squared_difference(tf.math.log1p(y_pred), tf.math.log1p(y_true))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfcQZ8r_8mF3",
        "outputId": "404f96ca-05d6-47d7-966b-fe3225c049dd"
      },
      "source": [
        "# Fiting the model - adjust hyperparameters as needed.\n",
        "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50)\n",
        "rlr_plateau = ReduceLROnPlateau(monitor='val_loss', factor=0.01, patience=10)\n",
        "checkpoint = ModelCheckpoint(checkpoint_path, save_best_only=True, monitor='val_loss', mode='min')\n",
        "\n",
        "model_cnnmlp.compile(optimizer='RMSProp', loss=RMSLE, metrics=['mean_squared_logarithmic_error'])\n",
        "model_cnnmlp.fit(x=generator_train, validation_data=generator_val, epochs=200, callbacks=[callback, checkpoint, rlr_plateau])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 13.6178 - RMSLE: 3.5378 - val_loss: 6.8976 - val_RMSLE: 2.5994\n",
            "Epoch 2/100\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 9.0194 - RMSLE: 2.9701 - val_loss: 6.9372 - val_RMSLE: 2.6057\n",
            "Epoch 3/100\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 8.4574 - RMSLE: 2.8788 - val_loss: 6.8962 - val_RMSLE: 2.5992\n",
            "Epoch 4/100\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 8.2442 - RMSLE: 2.8450 - val_loss: 6.9234 - val_RMSLE: 2.6034\n",
            "Epoch 5/100\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 8.0309 - RMSLE: 2.8066 - val_loss: 6.9293 - val_RMSLE: 2.6044\n",
            "Epoch 6/100\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 7.8966 - RMSLE: 2.7854 - val_loss: 6.9488 - val_RMSLE: 2.6075\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd9abcddeb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OgjgriamylN"
      },
      "source": [
        "model_cnnmlp.load_weights('cp3.hdf5')\r\n",
        "\r\n",
        "pred_test = model_cnnmlp.predict(generator_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBoP2p5HpTAu"
      },
      "source": [
        "d = np.array([test_set[[\"Id\"]].to_numpy(), pred_test]).reshape((2,2500)).transpose()\n",
        "res = pd.DataFrame(columns=['Id','Predicted'],index=None,data=d).set_index('Id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trNs3XDyZdB1"
      },
      "source": [
        "res.to_csv('submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePj0da00rkaS",
        "outputId": "481facdd-b63d-46dd-9980-3e1e21aa61ab"
      },
      "source": [
        "! kaggle competitions submit -c ift6758-a20 -f submission.csv -m \"without outliers and clustered color cleaned location\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100% 83.6k/83.6k [00:00<00:00, 198kB/s]\n",
            "Successfully submitted to Social Media Prediction"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9nPN-I1XrZZ"
      },
      "source": [
        "# res = np.array([test_set[[\"Id\"]].to_numpy(), pred_test]).reshape((2,2500)).transpose()\n",
        "# np.savetxt(\"foo.csv\", res, delimiter=\",\", fmt=\"%s\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}